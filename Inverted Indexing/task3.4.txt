{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf130
{\fonttbl\f0\fnil\fcharset0 Cambria;\f1\froman\fcharset0 Times-Roman;\f2\fnil\fcharset0 HelveticaNeue;
}
{\colortbl;\red255\green255\blue255;\red31\green42\blue46;\red17\green21\blue24;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid1\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl380\sa240\partightenfactor0
\ls1\ilvl0
\f0\fs32 \cf0 \expnd0\expndtw0\kerning0
Generate a stoplist using the unigram data. How would you choose your cut- off value? Briefly justify your choice and comment on the stoplist content. 
\f1\fs24 \uc0\u8232 \
\pard\tx720\pardeftab720\sl380\sa240\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0

\f2\fs28 \cf2 Some ways to choose cut off values and using the right stoplist content :-\
\
Low quality features in your training data-set is more likely to contribute negatively to your classification results (particularly since they might not be classified correctly), eliminating these low quality features can often lead to better classification results\
It is sometimes difficult to select a cut-off point for the most important features, generally it is recommended to design a research bench-work application that\'a0recursively\'a0tries different cut-off points and selects the one with the best accuracy (against a test data-set)\
\
Eliminate the some words which occur frequently:\
\pard\pardeftab720\li280\fi-280\partightenfactor0

\b \cf3 Frequently used English (or any language) words
\b0 \cf2 : this will include about 200 words that doesn\'92t really contribute to context, such as: and, if, the, for..etc\

\b Countries\
Cities\
Names\
Adjectives \
Temporal words: 
\b0 this will include about 100 words such as: Tuesday, Tomorrow, January ..etc.\
\
2 ) \
Look beyond unigram into bigram and trigrams\
\pard\pardeftab720\partightenfactor0
\cf2 They often considering bigrams in a classification algorithm tends to really boost performance, since the increased long-tail specificity of the word means that the classifier can easily determine which class has a higher probability, leading to better classifications. \
\
3) Tweak precision and recall\
The idea here is to tweak the system so when it fails, it does so in a manner that is more\'a0tolerable. \
\pard\pardeftab720\li280\fi-280\partightenfactor0
\cf2 \
}